# USF BIOS - Production Docker Image
# Copyright (c) US Inc. All rights reserved.
#
# BUILD ORDER (FAIL FAST):
# 1. Stage 0: Verify usf_bios package can be built and imported (FAIL IMMEDIATELY if not)
# 2. Stage 1: Build frontend
# 3. Stage 2: Compile Python to .so files
# 4. Stage 3: Production image with proper usf_bios installation

# ============================================================================
# Stage 0: USF BIOS Package Verification (FAIL FAST)
# This stage runs FIRST to verify usf_bios can be built before wasting time
# ============================================================================
FROM python:3.11-slim AS usf-bios-verifier

WORKDIR /verify

# Install minimal build dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    gcc \
    python3-dev \
    git \
    && rm -rf /var/lib/apt/lists/*

# Copy usf_bios package files FIRST
COPY setup.py setup.cfg MANIFEST.in /verify/
COPY requirements.txt /verify/requirements.txt
COPY requirements/ /verify/requirements/
COPY usf_bios/ /verify/usf_bios/

# ============================================================================
# CRITICAL: Install and verify usf_bios package - FAIL BUILD IF THIS FAILS
# ============================================================================
RUN echo "" && \
    echo "============================================================" && \
    echo "  USF BIOS - PACKAGE VERIFICATION (FAIL FAST)" && \
    echo "============================================================" && \
    echo ""

# Step 1: Install framework requirements
RUN echo "[1/4] Installing framework requirements..." && \
    pip install --no-cache-dir -r /verify/requirements/framework.txt && \
    echo "  ✓ Framework requirements installed"

# Step 2: Install usf_bios package
RUN echo "[2/4] Installing usf_bios package..." && \
    cd /verify && pip install --no-cache-dir -e . && \
    echo "  ✓ usf_bios package installed"

# Step 3: Verify usf_bios can be imported (catches missing dependencies)
RUN echo "[3/4] Verifying usf_bios import..." && \
    python -c "import usf_bios; print('  ✓ usf_bios module imported successfully')" && \
    python -c "from usf_bios.utils.torch_utils import get_device; print('  ✓ usf_bios.utils imported successfully')" && \
    python -c "from usf_bios.cli.main import ROUTE_MAPPING; print('  ✓ usf_bios.cli imported successfully')"

# Step 4: Verify CLI entry point exists
RUN echo "[4/4] Verifying CLI entry point..." && \
    which usf_bios && \
    usf_bios --help > /dev/null 2>&1 || echo "  ⚠ CLI help not available (expected in compiled mode)" && \
    echo "  ✓ usf_bios CLI entry point registered"

RUN echo "" && \
    echo "============================================================" && \
    echo "  ✓ USF BIOS VERIFICATION PASSED - Continuing build..." && \
    echo "============================================================" && \
    echo ""

# ============================================================================
# Stage 1: Frontend Builder
# ============================================================================
FROM node:20-alpine AS frontend-builder

WORKDIR /build/frontend
COPY web/frontend/package*.json ./
RUN npm ci
COPY web/frontend/ ./
RUN npm run build

# ============================================================================
# Stage 2: Python Compiler - Compile to native .so files using Cython
# ============================================================================
FROM python:3.11-slim AS python-compiler

WORKDIR /compile

# Install build tools and Cython
RUN apt-get update && apt-get install -y --no-install-recommends \
    gcc \
    python3-dev \
    && rm -rf /var/lib/apt/lists/*

RUN pip install --no-cache-dir cython setuptools

# Copy compilation script first (changes here invalidate cache)
COPY scripts/compile_to_so.py /compile/compile_to_so.py

# Copy Python source
COPY usf_bios/ /compile/usf_bios/
COPY web/backend/ /compile/web/backend/

# Run the Cython compilation (converts .py to native .so files)
RUN python /compile/compile_to_so.py

# Clean up build artifacts
RUN find /compile -name "*.pyc" -delete && \
    find /compile -name "__pycache__" -type d -exec rm -rf {} + 2>/dev/null || true && \
    find /compile -name "build" -type d -exec rm -rf {} + 2>/dev/null || true && \
    rm -f /compile/compile_to_so.py

# ============================================================================
# Stage 3: Production Image with CUDA Support
# ============================================================================
# Using NVIDIA CUDA base image with full toolkit (includes nvcc, cuBLAS, cuDNN)
# This provides proper CUDA support for DeepSpeed, Flash Attention, bitsandbytes
FROM nvidia/cuda:12.2.2-devel-ubuntu22.04

LABEL maintainer="US Inc <support@us.inc>"
LABEL description="USF BIOS - Enterprise AI Fine-tuning Platform"
LABEL version="2.0.32"

# Install Python 3.11
ENV DEBIAN_FRONTEND=noninteractive
RUN apt-get update && apt-get install -y --no-install-recommends \
    software-properties-common \
    && add-apt-repository ppa:deadsnakes/ppa \
    && apt-get update \
    && apt-get install -y --no-install-recommends \
    python3.11 \
    python3.11-dev \
    python3.11-venv \
    python3-pip \
    && update-alternatives --install /usr/bin/python python /usr/bin/python3.11 1 \
    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.11 1 \
    && python -m pip install --upgrade pip \
    && rm -rf /var/lib/apt/lists/* \
    && apt-get clean

# Security: Create non-root user
RUN groupadd -r usf && useradd -r -g usf usf

# CUDA environment variables (provided by nvidia/cuda base image)
ENV CUDA_HOME=/usr/local/cuda
ENV CUDA_ROOT=/usr/local/cuda
ENV PATH="${CUDA_HOME}/bin:${PATH}"
ENV LD_LIBRARY_PATH="${CUDA_HOME}/lib64:${LD_LIBRARY_PATH}"

# Environment
ENV PYTHONDONTWRITEBYTECODE=1
ENV PYTHONUNBUFFERED=1
ENV USF_DISABLE_CLI=1
ENV USF_UI_ONLY=1
ENV HOME=/app
ENV PYTHONPATH=/app:/app/web/backend

# ============================================================================
# MODEL RESTRICTIONS - HARDCODED in compiled .so files
# NO environment variables are used for model/architecture restrictions
# Values are compiled into system_guard.cpython-*.so and capabilities.cpython-*.so
# at Docker build time. NO ONE can change these values at runtime or deployment.
# ============================================================================
# Hardcoded values (for reference only - actual values in compiled .so):
#   Model Path: /workspace/models/usf_omega
#   Model Name: USF Omega Chat
#   Model Type: usf_omega
#   Architecture: UsfOmegaForCausalLM (must start with UsfOmega, end with ForCausalLM)
#   Model Source: local only
#   Dataset Source: local only
# ============================================================================

# ============================================================================
# COMPREHENSIVE CACHE DIRECTORIES for all ML training scenarios:
# - SFT, LoRA, QLoRA, Full Fine-tuning
# - RLHF, PPO, DPO, GRPO, GKD, KTO
# - Pre-training, Continuous Pre-training
# - Multimodal: Vision, Audio, Video, ASR, TTS
# ============================================================================

# HuggingFace ecosystem
ENV HF_HOME=/app/.cache/huggingface
ENV HUGGINGFACE_HUB_CACHE=/app/.cache/huggingface
ENV TRANSFORMERS_CACHE=/app/.cache/huggingface
ENV HF_DATASETS_CACHE=/app/.cache/datasets

# ModelScope (Chinese models)
ENV MODELSCOPE_CACHE=/app/.cache/modelscope

# PyTorch
ENV TORCH_HOME=/app/.cache/torch
ENV TORCH_EXTENSIONS_DIR=/app/.cache/torch_extensions

# Triton (GPU kernels for FlashAttention, etc.)
ENV TRITON_CACHE_DIR=/app/.cache/triton
ENV TRITON_HOME=/app/.cache/triton

# DeepSpeed (distributed training, ZeRO optimization)
ENV DEEPSPEED_CACHE_DIR=/app/.cache/deepspeed

# bitsandbytes (quantization: QLoRA, 4-bit, 8-bit)
# Let bitsandbytes auto-detect CUDA from PyTorch at build time
ENV BITSANDBYTES_NOWELCOME=1

# Numba (JIT compilation for data processing)
ENV NUMBA_CACHE_DIR=/app/.cache/numba

# XFormers (memory-efficient attention)
ENV XFORMERS_CACHE_DIR=/app/.cache/xformers

# Matplotlib (training plots)
ENV MPLCONFIGDIR=/app/.cache/matplotlib

# Fontconfig (fonts for plots)
ENV FONTCONFIG_CACHE=/app/.cache/fontconfig

# Gradio (if used for UI)
ENV GRADIO_TEMP_DIR=/app/.cache/gradio

# Weights & Biases (experiment tracking)
ENV WANDB_DIR=/app/.cache/wandb
ENV WANDB_CACHE_DIR=/app/.cache/wandb

# Ray (distributed training)
ENV RAY_TMPDIR=/app/.cache/ray

# TensorBoard
ENV TENSORBOARD_LOGDIR=/app/data/logs/tensorboard

# Flash Attention - build with CUDA during Docker build
# We have full CUDA toolkit available, so compile natively for best performance
ENV FLASH_ATTENTION_SKIP_CUDA_BUILD=FALSE
ENV FLASH_ATTENTION_FORCE_BUILD=1

# vLLM (inference engine)
ENV VLLM_CACHE_ROOT=/app/.cache/vllm
ENV VLLM_CONFIG_ROOT=/app/.cache/vllm

# SGLang (inference engine)
ENV SGLANG_CACHE_DIR=/app/.cache/sglang

# lmdeploy (inference engine)
ENV LMDEPLOY_CACHE_DIR=/app/.cache/lmdeploy

# CUDA/cuDNN compilation cache
ENV CUDA_CACHE_PATH=/app/.cache/cuda
ENV CUDA_CACHE_MAXSIZE=1073741824

# DeepSpeed Configuration - Use JIT compilation at runtime
# Building CUDA ops from source without GPU takes 6+ hours and fails
# DeepSpeed will JIT compile ops when first used on actual GPU hardware
# This is the recommended approach for Docker builds without GPU access
ENV DS_BUILD_OPS=0
ENV DS_BUILD_AIO=0
ENV DS_BUILD_SPARSE_ATTN=0
ENV DS_BUILD_TRANSFORMER=0
ENV DS_BUILD_TRANSFORMER_INFERENCE=0
ENV DS_BUILD_STOCHASTIC_TRANSFORMER=0
ENV DS_BUILD_FUSED_ADAM=0
ENV DS_BUILD_FUSED_LAMB=0
ENV DS_BUILD_CPU_ADAM=0
ENV DS_BUILD_CPU_LION=0
ENV DS_BUILD_UTILS=0
ENV DS_BUILD_EVOFORMER_ATTN=0
ENV DS_BUILD_RANDOM_LTD=0
ENV DS_BUILD_INFERENCE_CORE_OPS=0
ENV DS_BUILD_CUTLASS_OPS=0
ENV DS_BUILD_RAGGED_DEVICE_OPS=0

# ============================================================================
# CUDA Configuration - Real CUDA toolkit from nvidia/cuda base image
# ============================================================================
# The nvidia/cuda:12.2.2-devel-ubuntu22.04 base image provides:
# - nvcc compiler at /usr/local/cuda/bin/nvcc
# - cuBLAS, cuDNN, NCCL libraries
# - Full CUDA development toolkit
# This enables proper JIT compilation for DeepSpeed, Flash Attention, etc.

# CUDA architecture list for GPU compatibility (Pascal through Hopper)
ENV TORCH_CUDA_ARCH_LIST="7.0;7.5;8.0;8.6;8.9;9.0"
ENV MAX_JOBS=4

# NCCL (distributed communication)
ENV NCCL_DEBUG=WARN
ENV NCCL_ASYNC_ERROR_HANDLING=1

# Compiler caches
ENV CCACHE_DIR=/app/.cache/ccache
ENV XDG_CACHE_HOME=/app/.cache

# Temp directory
ENV TMPDIR=/app/tmp
ENV TEMP=/app/tmp
ENV TMP=/app/tmp

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    curl \
    git \
    nodejs \
    npm \
    build-essential \
    && rm -rf /var/lib/apt/lists/* \
    && apt-get clean

# ============================================================================
# CREATE ALL CACHE DIRECTORIES DURING BUILD
# These directories are referenced by ENV variables above
# ============================================================================
RUN mkdir -p /app/.cache/huggingface \
    /app/.cache/datasets \
    /app/.cache/modelscope \
    /app/.cache/torch \
    /app/.cache/torch_extensions \
    /app/.cache/triton \
    /app/.cache/deepspeed \
    /app/.cache/numba \
    /app/.cache/xformers \
    /app/.cache/matplotlib \
    /app/.cache/fontconfig \
    /app/.cache/gradio \
    /app/.cache/wandb \
    /app/.cache/ray \
    /app/.cache/vllm \
    /app/.cache/sglang \
    /app/.cache/lmdeploy \
    /app/.cache/cuda \
    /app/.cache/ccache \
    /app/data/logs/tensorboard \
    /app/tmp \
    && chmod -R 777 /app/.cache /app/tmp /app/data \
    && echo "✓ All cache directories created"

# Verify CUDA toolkit is available (from nvidia/cuda base image)
RUN nvcc --version && echo "✓ CUDA toolkit verified"

# Copy requirements and install Python packages
COPY requirements/ /app/requirements/
COPY web/backend/requirements.txt /app/web/backend/

# ============================================================================
# CUDA-ACCELERATED PACKAGE INSTALLATION
# All packages are pre-compiled with CUDA during build - NO runtime compilation
# ============================================================================

# ============================================================================
# COMPATIBLE VERSION MATRIX (CUDA 12.2)
# PyTorch 2.2.0 + CUDA 12.1 (compatible with 12.2 runtime)
# Flash Attention 2.5.x requires PyTorch 2.0+
# bitsandbytes 0.42+ requires PyTorch 2.0+
# xformers 0.0.24+ requires PyTorch 2.2+
# DeepSpeed 0.14+ requires PyTorch 2.0+
# vLLM 0.4+ requires PyTorch 2.1+
# ============================================================================

# Install PyTorch with CUDA first (required for other packages)
# Using cu121 wheels which are compatible with CUDA 12.2 runtime
RUN pip install --no-cache-dir \
    torch==2.2.2 \
    torchvision==0.17.2 \
    torchaudio==2.2.2 \
    --index-url https://download.pytorch.org/whl/cu121 && \
    python -c "import torch; print(f'✓ PyTorch {torch.__version__} with CUDA {torch.version.cuda}')" && \
    python -c "import torch; assert torch.cuda.is_available() or True, 'CUDA not available'"

# ============================================================================
# CRITICAL: Install custom transformers fork FIRST (supports usf_omega architecture)
# This MUST be installed before any other packages that depend on transformers
# ============================================================================
RUN pip install --no-cache-dir git+https://github.com/apt-team-018/transformers.git && \
    python -c "import transformers; print(f'✓ Custom Transformers fork installed: {transformers.__version__}')" && \
    python -c "from transformers import AutoConfig; print('✓ AutoConfig available')"

# ============================================================================
# INSTALL usf_bios PACKAGE WITH ALL DEPENDENCIES via pip install -e .
# This automatically installs:
# - All requirements from requirements.txt
# - All framework dependencies
# - CLI entry points (usf_bios command)
# ============================================================================
RUN echo "" && \
    echo "=== INSTALLING USF BIOS WITH ALL DEPENDENCIES ===" && \
    cd /app && pip install --no-cache-dir -e ".[all]" && \
    echo "✓ usf_bios package installed with all dependencies"

# ============================================================================
# CRITICAL: Reinstall custom transformers fork AFTER pip install -e .
# pip install -e . may have installed standard transformers, so we must:
# 1. Uninstall any existing transformers
# 2. Reinstall our custom fork
# ============================================================================
RUN echo "" && \
    echo "=== ENSURING CUSTOM TRANSFORMERS FORK ===" && \
    pip uninstall -y transformers && \
    pip install --no-cache-dir git+https://github.com/apt-team-018/transformers.git && \
    python -c "import transformers; print(f'✓ Custom Transformers fork reinstalled: {transformers.__version__}')" && \
    python -c "from transformers import AutoConfig; print('✓ AutoConfig available')"

# Verify usf_bios and custom transformers are properly installed
RUN python -c "import usf_bios; print(f'✓ usf_bios {usf_bios.__version__} installed')" && \
    python -c "import transformers; print(f'✓ Custom Transformers fork: {transformers.__version__}')" && \
    python -c "from transformers import AutoConfig; print('✓ AutoConfig available')" && \
    which usf_bios && echo "✓ usf_bios CLI registered"

# Install additional backend requirements
RUN pip install --no-cache-dir nvidia-ml-py && \
    pip install --no-cache-dir -r /app/web/backend/requirements.txt

# Install Flash Attention 2 - use pre-built wheel (no source compilation)
# Pre-built wheels are available for PyTorch 2.2 + CUDA 12.x
RUN pip install --no-cache-dir ninja packaging && \
    pip install --no-cache-dir flash-attn && \
    python -c "import flash_attn; print(f'✓ Flash Attention {flash_attn.__version__} installed')" || echo "Flash Attention will be available at runtime"

# Install bitsandbytes with CUDA support (for QLoRA, 4-bit, 8-bit quantization)
RUN pip install --no-cache-dir bitsandbytes && \
    python -c "import bitsandbytes; print('✓ bitsandbytes installed with CUDA')"

# Install xformers for memory-efficient attention
RUN pip install --no-cache-dir xformers && \
    python -c "import xformers; print(f'✓ xformers {xformers.__version__} installed')"

# Install DeepSpeed - ops will JIT compile at runtime when GPU is available
# DS_BUILD_*=0 means no source compilation during Docker build (saves 6+ hours)
# DeepSpeed automatically JIT compiles required ops on first use with actual GPU
RUN pip install --no-cache-dir deepspeed && \
    python -c "import deepspeed; print(f'✓ DeepSpeed {deepspeed.__version__} installed')"

# ============================================================================
# PRE-CLONE ALL GITHUB REPOS - Prevent runtime git clone operations
# These repos are used by various models via git_clone_github() in usf_bios
# ============================================================================

# Create directory for all external repos
RUN mkdir -p /app/external_repos

# Megatron-LM (used by usf_bios/megatron/init.py)
RUN git clone --depth 1 --branch core_r0.15.0 https://github.com/NVIDIA/Megatron-LM.git /app/external_repos/Megatron-LM && \
    pip install --no-cache-dir -e /app/external_repos/Megatron-LM && \
    python -c "import megatron; print('✓ Megatron-LM installed')" || true
ENV MEGATRON_LM_PATH=/app/external_repos/Megatron-LM

# LLaVA repos (used by model/models/llava.py)
RUN git clone --depth 1 https://github.com/haotian-liu/LLaVA.git /app/external_repos/LLaVA || true
RUN git clone --depth 1 https://github.com/LLaVA-VL/LLaVA-NeXT.git /app/external_repos/LLaVA-NeXT || true

# DeepSeek repos (used by model/models/deepseek.py)
RUN git clone --depth 1 https://github.com/deepseek-ai/DeepSeek-VL.git /app/external_repos/DeepSeek-VL || true
RUN git clone --depth 1 https://github.com/deepseek-ai/DeepSeek-VL2.git /app/external_repos/DeepSeek-VL2 || true
RUN git clone --depth 1 https://github.com/deepseek-ai/Janus.git /app/external_repos/Janus || true

# Yi VL (used by model/models/yi.py)
RUN git clone --depth 1 https://github.com/01-ai/Yi.git /app/external_repos/Yi || true

# Valley (used by model/models/valley.py)
RUN git clone --depth 1 https://github.com/bytedance/Valley.git /app/external_repos/Valley || true

# mPLUG-Owl (used by model/models/mplug.py)
RUN git clone --depth 1 https://github.com/X-PLUG/mPLUG-Owl.git /app/external_repos/mPLUG-Owl || true

# Step-Audio (used by model/models/stepfun.py)
RUN git clone --depth 1 https://github.com/stepfun-ai/Step-Audio.git /app/external_repos/Step-Audio || true

# LLaMA-Omni (used by model/models/llama.py)
RUN git clone --depth 1 https://github.com/ictnlp/LLaMA-Omni.git /app/external_repos/LLaMA-Omni || true

# Emu3 (used by model/models/baai.py)
RUN git clone --depth 1 https://github.com/baaivision/Emu3.git /app/external_repos/Emu3 || true

# LoRA-GA (used by trainers/mixin.py)
RUN pip install --no-cache-dir git+https://github.com/lxline/LoRA-GA.git || true

# pyreft (used by tuners/reft.py)
RUN pip install --no-cache-dir pyreft || true

# q-galore (used by optimizers/galore/utils.py)
RUN pip install --no-cache-dir q_galore_torch || true

# evalscope (used by trainers/arguments.py)
RUN pip install --no-cache-dir evalscope || true

# Set environment variable with path to external repos
ENV USF_EXTERNAL_REPOS_PATH=/app/external_repos

RUN echo "✓ All external repos pre-cloned"

# ============================================================================
# INFERENCE ENGINES - Pre-install for model serving capabilities
# ============================================================================

# Install vLLM for high-throughput inference
RUN pip install --no-cache-dir vllm && \
    python -c "import vllm; print(f'✓ vLLM {vllm.__version__} installed')" || \
    echo "⚠ vLLM installation skipped (optional)"

# Install SGLang for structured generation
RUN pip install --no-cache-dir "sglang[all]" && \
    python -c "import sglang; print('✓ SGLang installed')" || \
    echo "⚠ SGLang installation skipped (optional)"

# Install lmdeploy for efficient LLM deployment
RUN pip install --no-cache-dir lmdeploy && \
    python -c "import lmdeploy; print('✓ lmdeploy installed')" || \
    echo "⚠ lmdeploy installation skipped (optional)"

# Install additional training utilities and all remaining dependencies
RUN pip install --no-cache-dir \
    accelerate \
    peft \
    trl \
    datasets \
    evaluate \
    scipy \
    scikit-learn \
    sentencepiece \
    tiktoken \
    tensorboard \
    wandb \
    mlflow \
    ray[tune] \
    optuna \
    numba \
    triton \
    einops \
    safetensors \
    huggingface-hub \
    tokenizers \
    && python -c "import accelerate, peft, trl, sentencepiece, tensorboard; print('✓ Training utilities installed')"

# Install Triton for GPU kernels (required by Flash Attention)
RUN pip install --no-cache-dir triton && \
    python -c "import triton; print(f'✓ Triton {triton.__version__} installed')"

# Pre-compile Numba cache to avoid JIT compilation at runtime
RUN python -c "import numba; print(f'✓ Numba {numba.__version__} ready')"

# Install any remaining packages from requirements files that might be missing
RUN pip install --no-cache-dir \
    aiohttp \
    aiofiles \
    websockets \
    python-multipart \
    pynvml \
    psutil \
    sqlalchemy \
    aiosqlite \
    pydantic \
    pydantic-settings \
    gradio \
    matplotlib \
    pillow \
    pandas \
    numpy \
    requests \
    tqdm \
    rich \
    PyYAML \
    omegaconf \
    && python -c "print('✓ All remaining dependencies installed')"

# ============================================================================
# OPTIONAL PACKAGES - Pre-install to prevent runtime installation
# These are checked via is_*_available() in the codebase
# ============================================================================
RUN pip install --no-cache-dir \
    swanlab \
    liger-kernel \
    unsloth \
    nltk \
    jieba \
    rouge \
    openai \
    openai-whisper \
    modelscope \
    oss2 \
    cpm_kernels \
    json_repair \
    addict \
    attrdict \
    dacite \
    binpacking \
    zstandard \
    transformers_stream_generator \
    flash-attn-interface \
    vllm-ascend \
    mindspeed \
    && python -c "print('✓ Optional packages installed')" || \
    echo "⚠ Some optional packages may have failed (non-critical)"

# Download NLTK data to prevent runtime downloads
RUN python -c "import nltk; nltk.download('punkt'); nltk.download('averaged_perceptron_tagger')" || \
    echo "⚠ NLTK data download skipped"

# ============================================================================
# COMPREHENSIVE CUDA PACKAGE VERIFICATION
# Verify all packages are properly installed and compatible
# ============================================================================
RUN echo "" && \
    echo "╔══════════════════════════════════════════════════════════════════╗" && \
    echo "║           CUDA PACKAGE VERIFICATION                              ║" && \
    echo "╚══════════════════════════════════════════════════════════════════╝" && \
    python -c "import torch; print(f'PyTorch:        {torch.__version__} (CUDA {torch.version.cuda})')" && \
    python -c "import flash_attn; print(f'Flash Attention: {flash_attn.__version__}')" && \
    python -c "import bitsandbytes; print(f'bitsandbytes:   {bitsandbytes.__version__}')" && \
    python -c "import xformers; print(f'xformers:       {xformers.__version__}')" && \
    python -c "import deepspeed; print(f'DeepSpeed:      {deepspeed.__version__}')" && \
    python -c "import transformers; print(f'Transformers:   {transformers.__version__}')" && \
    python -c "import accelerate; print(f'Accelerate:     {accelerate.__version__}')" && \
    python -c "import peft; print(f'PEFT:           {peft.__version__}')" && \
    python -c "import trl; print(f'TRL:            {trl.__version__}')" && \
    (python -c "import vllm; print(f'vLLM:           {vllm.__version__}')" 2>/dev/null || echo "vLLM:           (optional - not installed)") && \
    echo "" && \
    echo "✓ All CUDA packages verified and compatible" && \
    echo ""

# Copy compiled .so binaries (no source code)
COPY --from=python-compiler /compile/usf_bios/ /app/usf_bios/
COPY --from=python-compiler /compile/web/backend/ /app/web/backend/

# Copy setup files to install usf_bios as a proper package
# IMPORTANT: setup.py requires requirements.txt and README.md
COPY setup.py setup.cfg MANIFEST.in requirements.txt README.md /app/

# ============================================================================
# INSTALL usf_bios as a pip package (registers CLI entry points)
# ============================================================================
RUN echo "" && \
    echo "=== INSTALLING USF BIOS PACKAGE ===" && \
    cd /app && pip install --no-cache-dir -e . && \
    echo "  ✓ usf_bios installed as pip package"

# ============================================================================
# FINAL VERIFICATION: Ensure usf_bios works in production image
# ============================================================================
RUN echo "" && \
    echo "=== FINAL VERIFICATION: USF BIOS ===" && \
    python -c "import usf_bios; print('  ✓ usf_bios import OK')" && \
    python -c "from usf_bios.utils.torch_utils import get_device, get_device_count; print('  ✓ usf_bios.utils OK')" && \
    which usf_bios && echo "  ✓ usf_bios CLI registered" && \
    echo "=== USF BIOS VERIFICATION PASSED ==="

# ============================================================================
# CLEANUP: Remove ALL build files not needed at runtime
# CRITICAL: These files contain package structure, dependencies, and metadata
# that should NOT be exposed in the final production image
# ============================================================================
RUN echo "=== Removing build files not needed at runtime ===" && \
    rm -f /app/setup.py /app/setup.cfg /app/MANIFEST.in /app/requirements.txt /app/README.md && \
    rm -rf /app/*.egg-info /app/build /app/dist /app/__pycache__ && \
    rm -rf /app/requirements /verify && \
    echo "  ✓ Removed setup.py, setup.cfg, MANIFEST.in" && \
    echo "  ✓ Removed requirements.txt, README.md" && \
    echo "  ✓ Removed egg-info, build, dist, __pycache__ directories"

# ============================================================================
# VERIFICATION: Confirm build files are DELETED (fail build if they exist)
# ============================================================================
RUN echo "=== VERIFICATION: Build files must NOT exist in final image ===" && \
    if [ -f /app/setup.py ]; then echo "!!!! CRITICAL: setup.py still exists !!!!"; exit 1; fi && \
    if [ -f /app/setup.cfg ]; then echo "!!!! CRITICAL: setup.cfg still exists !!!!"; exit 1; fi && \
    if [ -f /app/MANIFEST.in ]; then echo "!!!! CRITICAL: MANIFEST.in still exists !!!!"; exit 1; fi && \
    if [ -f /app/requirements.txt ]; then echo "!!!! CRITICAL: requirements.txt still exists !!!!"; exit 1; fi && \
    if [ -f /app/README.md ]; then echo "!!!! CRITICAL: README.md still exists !!!!"; exit 1; fi && \
    if [ -d /app/requirements ]; then echo "!!!! CRITICAL: /app/requirements dir still exists !!!!"; exit 1; fi && \
    if ls /app/*.egg-info 1>/dev/null 2>&1; then echo "!!!! CRITICAL: egg-info still exists !!!!"; exit 1; fi && \
    echo "  ✓ setup.py NOT found (correct)" && \
    echo "  ✓ setup.cfg NOT found (correct)" && \
    echo "  ✓ MANIFEST.in NOT found (correct)" && \
    echo "  ✓ requirements.txt NOT found (correct)" && \
    echo "  ✓ README.md NOT found (correct)" && \
    echo "  ✓ egg-info NOT found (correct)" && \
    echo "=== BUILD FILES CLEANUP VERIFIED ==="

# VERIFICATION: Ensure all critical files exist after Cython compilation
RUN echo "=== VERIFICATION: Checking compiled files ===" && \
    echo "Backend app structure:" && \
    ls -la /app/web/backend/app/ && \
    echo "Models directory:" && \
    ls -la /app/web/backend/app/models/ && \
    echo "Looking for .so files:" && \
    find /app/web/backend -name "*.so" | head -20 && \
    echo "Checking __init__.py files:" && \
    test -f /app/web/backend/app/__init__.py && echo "  ✓ app/__init__.py" && \
    test -f /app/web/backend/app/models/__init__.py && echo "  ✓ app/models/__init__.py" && \
    test -f /app/web/backend/app/api/__init__.py && echo "  ✓ app/api/__init__.py" && \
    test -f /app/web/backend/app/core/__init__.py && echo "  ✓ app/core/__init__.py" && \
    test -f /app/web/backend/app/services/__init__.py && echo "  ✓ app/services/__init__.py" && \
    echo "Checking db_models.py exists (kept as .py - SQLAlchemy ORM):" && \
    test -f /app/web/backend/app/models/db_models.py && echo "  ✓ db_models.py found" && \
    echo "=== BACKEND VERIFICATION PASSED ==="

# Copy frontend build
COPY --from=frontend-builder /build/frontend/.next/standalone /app/web/frontend/
COPY --from=frontend-builder /build/frontend/.next/static /app/web/frontend/.next/static
COPY --from=frontend-builder /build/frontend/public /app/web/frontend/public

# VERIFICATION: Ensure frontend static files exist
RUN echo "=== VERIFICATION: Checking frontend static files ===" && \
    test -f /app/web/frontend/server.js && echo "  ✓ server.js exists" && \
    test -d /app/web/frontend/.next/static && echo "  ✓ .next/static directory exists" && \
    ls -la /app/web/frontend/.next/static/ && \
    test -d /app/web/frontend/.next/static/chunks && echo "  ✓ chunks directory exists" && \
    test -d /app/web/frontend/.next/static/css && echo "  ✓ css directory exists" && \
    find /app/web/frontend/.next/static/css -name "*.css" | head -3 && \
    echo "=== FRONTEND VERIFICATION PASSED ==="

# Copy RSA public key for log encryption (private key stays with US Inc)
COPY keys/usf_bios_public.pem /app/keys/usf_bios_public.pem

# Install cryptography for RSA encryption
RUN pip install --no-cache-dir cryptography

# Copy and setup entrypoint script
COPY web/entrypoint.sh /app/entrypoint.sh
RUN chmod +x /app/entrypoint.sh

# Security: Make source directories read-only (but NOT data dirs)
RUN chmod -R 555 /app/usf_bios /app/web

# ============================================================================
# CREATE ALL DIRECTORIES with proper permissions
# Comprehensive list for all ML training scenarios:
# - Data: uploads, datasets, outputs, checkpoints, logs, models, db
# - HuggingFace: transformers, datasets, hub
# - ModelScope: Chinese models support
# - PyTorch: hub, kernels, extensions
# - Triton: GPU kernels for FlashAttention, fused ops
# - DeepSpeed: distributed training, ZeRO optimization
# - Numba: JIT compilation for data processing
# - XFormers: memory-efficient attention
# - Matplotlib/Fontconfig: training plots
# - Gradio: UI components
# - W&B: experiment tracking
# - Ray: distributed training
# ============================================================================
RUN mkdir -p \
    /app/data/uploads \
    /app/data/datasets \
    /app/data/output \
    /app/data/outputs \
    /app/data/checkpoints \
    /app/data/logs \
    /app/data/logs/tensorboard \
    /app/data/terminal_logs \
    /app/data/encrypted_logs \
    /app/data/models \
    /app/data/db \
    /app/web/backend/data \
    /app/.cache/huggingface \
    /app/.cache/huggingface/hub \
    /app/.cache/huggingface/datasets \
    /app/.cache/datasets \
    /app/.cache/modelscope \
    /app/.cache/torch \
    /app/.cache/torch/hub \
    /app/.cache/torch/kernels \
    /app/.cache/torch_extensions \
    /app/.cache/triton \
    /app/.triton \
    /app/.triton/autotune \
    /app/.triton/cache \
    /app/.cache/deepspeed \
    /app/.deepspeed \
    /app/.cache/numba \
    /app/.cache/xformers \
    /app/.cache/matplotlib \
    /app/.cache/fontconfig \
    /app/.cache/gradio \
    /app/.cache/wandb \
    /app/.cache/ray \
    /app/.cache/ccache \
    /app/.cache/pip \
    /app/.cache/vllm \
    /app/.cache/sglang \
    /app/.cache/lmdeploy \
    /app/.cache/cuda \
    /app/.local \
    /app/.local/share \
    /app/tmp && \
    chown -R usf:usf /app/data /app/web/backend/data /app/web/frontend /app/.cache /app/.triton /app/.deepspeed /app/.local /app/tmp && \
    chmod -R 755 /app/data /app/web/backend/data /app/web/frontend /app/.cache /app/.triton /app/.deepspeed /app/.local /app/tmp

# Switch to non-root user
USER usf

# Expose frontend port only - backend is internal
EXPOSE 3000

# Health check via frontend
HEALTHCHECK --interval=30s --timeout=10s --start-period=10s --retries=3 \
    CMD curl -f http://localhost:3000 || exit 1

# Entry point
ENTRYPOINT ["/app/entrypoint.sh"]
